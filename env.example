# LLMLite Proxy Configuration
# This example shows how to configure LangChain to use llmlite as a proxy
# for Claude 3.5 Sonnet instead of calling Anthropic directly

# Copy this file to .env and fill in your actual values:
# cp env.example .env

# LLMLite proxy URL (default: http://localhost:8080)
LLMLITE_BASE_URL=http://localhost:8080/v1

# LLMLite API key (get this from your llmlite instance)
LLMLITE_API_KEY=your-llmlite-api-key-here

# Model to use through llmlite (Claude 3.5 Sonnet)
# Note: llmlite may map this to different provider-specific model names
LLMLITE_MODEL=claude-3-5-sonnet-latest

# Alternative Configuration Options:
# ================================

# Option 1: If llmlite exposes OpenAI-compatible endpoints
# Uncomment and use these instead:
# OPENAI_BASE_URL=http://localhost:8080/v1
# OPENAI_API_KEY=your-llmlite-api-key-here

# Option 2: If using llmlite with Anthropic-compatible endpoint
# Uncomment and use these instead:
# ANTHROPIC_BASE_URL=http://localhost:8080
# ANTHROPIC_API_KEY=your-llmlite-api-key-here

# Optional: Logging configuration
# LOG_LEVEL=INFO

